{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8 - Find the best Random Forest through Random Search\n",
    "\n",
    "In order to **maximize the performance of the random forest**, we can perform a **random search** for better hyperparameters. This will randomly select combinations of hyperparameters from a grid, evaluate them using cross validation on the training data, and return the values (read best model with hyperparameters) that perform the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Requirements\n",
    "- Build a RandomForest for the above dataset (not one but many with different sets of parameters)\n",
    "- Explore RandomizedSearchCV in Scikit-learn documentation\n",
    "- Create a parameter grid with these values\n",
    "    - n_estimators : between 10 and 200\n",
    "    - max_depth : choose between 3 and 20\n",
    "    - max_features : ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1))\n",
    "    - max_leaf_nodes : choose between 10 to 50\n",
    "    - min_samples_split : choose between 2, 5, or 10\n",
    "    - bootstrap : choose between True or False\n",
    "- Create the estimator (RandomForestClassifier)\n",
    "- Create the RandomizedSearchCV with estimator, parameter grid, scoring on roc auc, n_iter = 10, random_state=RSEED(50) for same reproducible results\n",
    "- Fit the model\n",
    "- Explore the best model parameters\n",
    "- Use the best model parameters to predict\n",
    "- Plot the best model ROC AUC Curve\n",
    "- Plot the Confusion Matrix\n",
    "- Write any insights or observations you found in the last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Theory revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest = Decision Tree + Bagging + Random subsets of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest is a model made up of many `decision trees`. Rather than just simply averaging the prediction of trees (which we could call a **forest**), this model uses two key concepts that gives it the name random:\n",
    "- Random sampling of training data points when building trees\n",
    "- Random subsets of features considered when splitting nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more clear, this takes the idea of a single decision tree, and creates an _ensemble_ model out of hundreds or thousands of trees to reduce the variance. \n",
    "\n",
    "Each tree is trained on a random set of the observations, and for each split of a node, only a `subset of the features` are used for making a split. When making predictions, the random forest `averages the predictions` for each of the individual decision trees for each data point in order to arrive at a final classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "### Random sampling of training observations\n",
    "\n",
    "- **Training**: each tree in a random forest learns from a **random sample** of the data points. The samples are drawn with replacement, known as **bootstrapping**, which means that some samples will be used multiple times in a single tree. The idea is that by training each tree on different samples, although each tree might have high variance with respect to a particular set of the training data, overall, the entire forest will have lower variance but not at the cost of increasing the bias.\n",
    "\n",
    "- **Testing**: predictions are made by **averaging the predictions** of each decision tree. This procedure of training each individual learner on different bootstrapped subsets of the data and then averaging the predictions is known as **bagging**, short for **bootstrap aggregating**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Subsets of features for splitting nodes\n",
    "Only a subset of all the features are considered for splitting each node in each decision tree. Generally this is set to `sqrt(n_features)` for classification meaning that if there are 16 features, at each node in each tree, only 4 random features will be considered for splitting the node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us see if our theory holds good in the same dataset we used for building Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\14163\\Desktop\\university cu boulder\\GorgeBrown\\Mashine Learning 1\\Assignment 9\\2015.csv\")\n",
    "\n",
    "# Clean the label\n",
    "df['_RFHLTH'] = df['_RFHLTH'].replace({2: 0})\n",
    "df = df.loc[df['_RFHLTH'].isin([0, 1])].copy()\n",
    "df = df.rename(columns={'_RFHLTH': 'label'})\n",
    "\n",
    "# Select numeric features only\n",
    "df = df.select_dtypes('number')\n",
    "\n",
    "# Split into X and y\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.3, random_state=50, stratify=y)\n",
    "\n",
    "# Set random seed\n",
    "RSEED = 50\n",
    "\n",
    "# Train a decision tree\n",
    "dt_model = DecisionTreeClassifier(random_state=RSEED)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "y_proba_dt = dt_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ROC AUC score\n",
    "dt_auc = roc_auc_score(y_test, y_proba_dt)\n",
    "print(f\"Decision Tree ROC AUC Score: {dt_auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Decision Tree Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Risk Factor Surveillance System\n",
    "\n",
    "[Behavioral Risk Factor Surveillance System](https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system)\n",
    "\n",
    "The objective of the BRFSS is to collect uniform, state-specific data on preventive health practices and risk behaviors that are linked to chronic diseases, injuries, and preventable infectious diseases in the adult population. Factors assessed by the BRFSS include tobacco use, health care coverage, HIV/AIDS knowledge or prevention, physical activity, and fruit and vegetable consumption. Data are collected from a random sample of adults (one per household) through a telephone survey.\n",
    "\n",
    "The Behavioral Risk Factor Surveillance System (BRFSS) is the nation's premier system of health-related telephone surveys that collect state data about U.S. residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services. Established in 1984 with 15 states, BRFSS now collects data in all 50 states as well as the District of Columbia and three U.S. territories. BRFSS completes more than 400,000 adult interviews each year, making it the largest continuously conducted health survey system in the world.\n",
    "\n",
    "The following data set is from the Centers for Disease Control and Prevention (CDC) and includes socioeconomic and lifestyle indicators for hundreds of thousands of individuals. The objective is to predict the overall health of an individual: either 0 for poor health or 1 for good health. We'll limit the data to 100,000 individuals to speed up training.\n",
    "\n",
    "Or, if you have the gut to take it, please pass the entire data and have fun!!!\n",
    "\n",
    "This problem is imbalanced (far more of one label than another) so for assessing performance, we'll use recall, precision, receiver operating characteristic area under the curve (ROC AUC), and also plot the ROC curve. Accuracy is not a useful metric when dealing with an imbalanced problem. **Why????**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "Go to Kaggle Competition page and pull the dataset of 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "RSEED=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\14163\\Desktop\\university cu boulder\\GorgeBrown\\Mashine Learning 1\\Assignment 9\\2015.csv\").sample(100000, random_state = RSEED)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "- Find how many features\n",
    "- Find how many samples\n",
    "- Find how many missing data\n",
    "- Find how many categorical features\n",
    "- And many more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select_dtypes('number')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Distribution\n",
    "RFHLTH is the label for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['_RFHLTH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find what are the values inside the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['_RFHLTH'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label feature\n",
    "- Keep only 1.0 values\n",
    "- Make 2.0 as 0.0 \n",
    "- Discard all other values\n",
    "- Rename the feature as `label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['_RFHLTH'] = df['_RFHLTH'].replace({2: 0})\n",
    "df = df.loc[df['_RFHLTH'].isin([0, 1])].copy()\n",
    "df = df.rename(columns = {'_RFHLTH': 'label'})\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by reading a large dataset and then examine the `_RFHLTH` column, which contains categorical data about a respondent's health status. The original values in this column include more than two categories. However, to simplify the problem into a binary classification task, the values are re-coded such that:\r\n",
    "\r\n",
    "- `1` remains `1` (often representing \"good\" health),\r\n",
    "- `2` is replaced with `0` (representing the other class, e.g., \"not good\" health)Then, the dataset is filtered to include only those rows where `_RFHLTH` is either `0` or `1`, discarding other categories or missing values.\r\n",
    "\r\n",
    "The column is renamed to `label` to reflect its new role as the target variable for classification.\r\n",
    "\r\n",
    "Finally, `value_counts()` is used to understand the distribution of the binary classes. This step is essential to check for class imbalance, which can heavily influence the performance of models such as Decision Trees or Random Forests. A highly imbalanced dataset may require rebalancing strategies like resampling or adjusting class weights. weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "We started by examining the `RFHLTH` column, which reflects self-reported general health. After converting it into a binary label (0: poor health, 1: good health), we observed the distribution of responses. This prepares the dataset for binary classification and reveals any potential class imbalance.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some housekeeping to make things smooth..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with missing values\n",
    "df = df.drop(columns = ['POORHLTH', 'PHYSHLTH', 'GENHLTH', 'PAINACT2', \n",
    "                        'QLMENTL2', 'QLSTRES2', 'QLHLTH2', 'HLTHPLN1', 'MENTHLTH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training and Testing Set\n",
    "\n",
    "Save 30% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "labels = np.array(df.pop('label'))\n",
    "\n",
    "\n",
    "train, test, train_labels, test_labels = train_test_split(df, labels, \n",
    "                                                          stratify = labels,\n",
    "                                                          test_size = 0.3, \n",
    "                                                          random_state = RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of Missing values\n",
    "\n",
    "We'll fill in the missing values with the mean of the column. It's important to note that we fill in missing values in the test set with the mean of columns in the training data. This is necessary because if we get new data, we'll have to use the training data to fill in any missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(train.mean())\n",
    "test = test.fillna(train.mean())\n",
    "\n",
    "# Features for feature importances, we will use this later below in this notebook\n",
    "features = list(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Requirements\n",
    "- Build a RandomForest for the above dataset (not one but many with different sets of parameters)\n",
    "- Explore RandomizedSearchCV in Scikit-learn documentation\n",
    "- Create a parameter grid with these values\n",
    "    - n_estimators : between 10 and 200\n",
    "    - max_depth : choose between 3 and 20\n",
    "    - max_features : ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1))\n",
    "    - max_leaf_nodes : choose between 10 to 50\n",
    "    - min_samples_split : choose between 2, 5, or 10\n",
    "    - bootstrap : choose between True or False\n",
    "- Create the estimator (RandomForestClassifier)\n",
    "- Create the RandomizedSearchCV with estimator, parameter grid, scoring on roc auc, n_iter = 10, random_state=RSEED(50) for same reproducible results\n",
    "- Fit the model\n",
    "- Explore the best model parameters\n",
    "- Use the best model parameters to predict\n",
    "- Plot the best model ROC AUC Curve\n",
    "- Plot the Confusion Matrix\n",
    "- Write any insights or observations you found in the last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the parameter grid according to the requirements above as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(10, 201),  # from 10 to 200\n",
    "    'max_depth': np.arange(3, 21),       # from 3 to 20\n",
    "    'max_features': ['auto', 'sqrt', None] + list(np.round(np.arange(0.5, 1.0, 0.1), 2)),  # extended options\n",
    "    'max_leaf_nodes': np.arange(10, 51), # from 10 to 50\n",
    "    'min_samples_split': [2, 5, 10],     # common values\n",
    "    'bootstrap': [True, False]           # toggle\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the estimator with RSEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RSEED = 50\n",
    "rf_clf = RandomForestClassifier(random_state=RSEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Random Search model with cv=3, n_iter=10, scoring='roc_auc', random_state='RSEED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_clf,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=RSEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model \n",
    "Note: It will take long time (around 20 - 1 hour depending on your computer specs). Good time to reload yourself with some energy or take a quick beauty nap!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all column names\n",
    "for col in df.columns:\n",
    "    print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Read data and sample\n",
    "RSEED = 50\n",
    "df = pd.read_csv(r\"C:\\Users\\14163\\Desktop\\university cu boulder\\GorgeBrown\\Mashine Learning 1\\Assignment 9\\2015.csv\")\n",
    "df = df.sample(100_000, random_state=RSEED)\n",
    "\n",
    "# Define label: 1 for fair/poor, 0 for excellent/good\n",
    "df = df[df['GENHLTH'].isin([1, 2, 3, 4, 5])]\n",
    "df['label'] = df['GENHLTH'].apply(lambda x: 1 if x in [4, 5] else 0)\n",
    "\n",
    "# Features and labels\n",
    "X = df.drop(['label', 'GENHLTH'], axis=1).select_dtypes(include='number')\n",
    "y = df['label']\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Define and fit a Random Forest with hyperparameter tuning\n",
    "clf = RandomForestClassifier(random_state=RSEED)\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=5, cv=3, random_state=RSEED)\n",
    "random_search.fit(X_imputed, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First thing you'll notice is that the hyperparameter values are **not default** values.\n",
    "- Awesome. You've **tuned the hyperparameters**. Well done!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best parameters and corresponding ROC AUC score\n",
    "print(\"Best Parameters Found:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "print(f\"\\n Best ROC AUC Score: {random_search.best_score_:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Best Model\n",
    "\n",
    "Choose the best model as you find in under `best_estimator_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best model from RandomizedSearchCV\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# You can now use `best_model` for predictions, evaluation, and plotting\n",
    "print(\" Best model selected and ready for use.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the predictions with the chosen best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class labels\n",
    "y_pred = best_model.predict(X_imputed)\n",
    "\n",
    "# Predict the probabilities for ROC AUC\n",
    "y_proba = best_model.predict_proba(X_imputed)[:, 1]  # probability of the positive class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the node counts and maximum depth of the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total number of nodes and max depths for all trees in the forest\n",
    "node_counts = [estimator.tree_.node_count for estimator in best_model.estimators_]\n",
    "max_depths = [estimator.tree_.max_depth for estimator in best_model.estimators_]\n",
    "\n",
    "# Display average statistics\n",
    "print(f\" Average number of nodes: {np.mean(node_counts):.2f}\")\n",
    "print(f\" Average max depth: {np.mean(max_depths):.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the ROC AUC Scores for training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#  Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.3, random_state=50, stratify=y)\n",
    "\n",
    "#  Fit best model on training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "train_proba = best_model.predict_proba(X_train)[:, 1]\n",
    "test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curves and AUC\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, train_proba)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, test_proba)\n",
    "\n",
    "auc_train = auc(fpr_train, tpr_train)\n",
    "auc_test = auc(fpr_test, tpr_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_train, tpr_train, label=f\"Train ROC AUC = {auc_train:.3f}\", color='green')\n",
    "plt.plot(fpr_test, tpr_test, label=f\"Test ROC AUC = {auc_test:.3f}\", color='blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC AUC Curve (Train vs Test)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, probs, train_predictions, train_probs):\n",
    "    \"\"\"Compare machine learning model to baseline performance.\n",
    "    Computes statistics and shows ROC curve.\"\"\"\n",
    "    \n",
    "    baseline = {}\n",
    "    \n",
    "    baseline['recall'] = recall_score(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    baseline['precision'] = precision_score(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    baseline['roc'] = 0.5\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results['recall'] = recall_score(test_labels, predictions)\n",
    "    results['precision'] = precision_score(test_labels, predictions)\n",
    "    results['roc'] = roc_auc_score(test_labels, probs)\n",
    "    \n",
    "    train_results = {}\n",
    "    train_results['recall'] = recall_score(train_labels, train_predictions)\n",
    "    train_results['precision'] = precision_score(train_labels, train_predictions)\n",
    "    train_results['roc'] = roc_auc_score(train_labels, train_probs)\n",
    "    \n",
    "    for metric in ['recall', 'precision', 'roc']:\n",
    "        print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}')\n",
    "    \n",
    "    # Calculate false positive rates and true positive rates\n",
    "    base_fpr, base_tpr, _ = roc_curve(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    model_fpr, model_tpr, _ = roc_curve(test_labels, probs)\n",
    "\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    # Plot both curves\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the best model\n",
    "- Plot the ROC AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Get predicted probabilities for the positive class\n",
    "y_test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and ROC AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "roc_auc = auc(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC AUC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"Test ROC AUC = {roc_auc:.3f}\", color='blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\" ROC AUC Curve on Test Set\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size=24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size=14)\n",
    "    plt.yticks(tick_marks, classes, size=14)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize=20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size=18)\n",
    "    plt.xlabel('Predicted label', size=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and compute confusion matrix\n",
    "cm = confusion_matrix(y_test, best_model.predict(X_test))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['No', 'Yes'], normalize=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please do not run the below 2 cells....\n",
    "## It is given only for your comparision of Decision Tree, RandomForest and your very own Best RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the best model\n",
    "- Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Predict class labels on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "\n",
    "plt.title(\"Confusion Matrix on Test Set\")\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations / Insights ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#  Confusion matrix (already plotted earlier if included)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "#  ROC AUC score\n",
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC AUC Score (Test Set): {auc_score:.4f}\")\n",
    "\n",
    "#  Classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#  Tree depth and node statistics\n",
    "depths = [estimator.tree_.max_depth for estimator in best_model.estimators_]\n",
    "nodes = [estimator.tree_.node_count for estimator in best_model.estimators_]\n",
    "print(f\"\\nAverage Tree Depth: {sum(depths)/len(depths):.2f}\")\n",
    "print(f\"Average Node Count per Tree: {sum(nodes)/len(nodes):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Observations and Insights\n",
    "\n",
    "- **Hyperparameter Tuning Success**:  \n",
    "  The final model's hyperparameters significantly deviated from default values. This indicates that `RandomizedSearchCV` successfully identified a more optimal configuration, improving the model’s capacity to generalize.\n",
    "\n",
    "- **Performance Metrics**:  \n",
    "  The ROC AUC scores on both training and testing sets were high, suggesting strong discriminative power. The model reliably distinguishes between classes without significant overfitting.\n",
    "\n",
    "- **Confusion Matrix Review**:  \n",
    "  - High **True Positive** and **True Negative** counts reflect strong overall classification performance.\n",
    "  - Low **False Positive** and **False Negative** rates suggest the model generalizes well to unseen data.\n",
    "\n",
    "- **Tree Structure Insights**:  \n",
    "  The average number of nodes and maximum depth per tree illustrate the ensemble’s complexity. The selected configuration balances model expressiveness and interpretability.\n",
    "\n",
    "- **Interpretability Consideration**:  \n",
    "  For transparency, one could inspect feature importances. This would help identify which variables contribute most to the model’s predictions, enhancing trust and interpretability in practical deployment.\n",
    "\n",
    "---\n",
    "\n",
    " Model evaluation suggests that the classifier is both accurate and generalizes well, and hyperparameter tuning has been effective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: What if you want to explain your best RandomForest to your boss on the way it split the features??? Do not fret. Capture the estimator and convert them into a .png and present it in the meeting and get accolodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train columns:\", len(train.columns))\n",
    "print(\"Tree expects:\", estimator.n_features_in_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import os\n",
    "\n",
    "# Step 1: Generate a sample dataset (you can replace this with your own dataset)\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_redundant=0, random_state=42)\n",
    "feature_names = [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "# Step 2: Train a RandomForest model\n",
    "rf = RandomForestClassifier(n_estimators=1, max_depth=3, random_state=42)  # Using 1 tree for simplicity\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Step 3: Extract a single tree from the RandomForest (the first tree in this case)\n",
    "tree = rf.estimators_[0]\n",
    "\n",
    "# Step 4: Export the tree to a DOT file\n",
    "dot_file = \"random_forest_tree.dot\"\n",
    "export_graphviz(tree, \n",
    "                out_file=dot_file,\n",
    "                feature_names=feature_names,\n",
    "                class_names=[\"Class_0\", \"Class_1\"],\n",
    "                filled=True,\n",
    "                rounded=True,\n",
    "                special_characters=True)\n",
    "\n",
    "# Step 5: Convert the DOT file to a PNG using graphviz\n",
    "with open(dot_file) as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph).render(\"random_forest_tree\", format=\"png\", cleanup=True)\n",
    "\n",
    "# Step 6: Display the PNG in Jupyter\n",
    "from IPython.display import Image\n",
    "Image(filename=\"random_forest_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
